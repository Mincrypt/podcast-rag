# ğŸ™ï¸ Audioâ€‘toâ€‘Text RAG for Podcast Search

A lightweight multimodal RAG system that ingests audio podcasts, transcribes them with timestamps, builds a searchable index across multiple episodes, and answers topic queries with timeâ€‘coded references.

**Demo stack:** Streamlit + fasterâ€‘whisper + Sentenceâ€‘Transformers + ChromaDB (+ optional pyannote diarization, + optional Groq/OpenAI LLM).

## Features

- Upload multiple podcast episodes (`.mp3/.wav/.m4a`).
- Highâ€‘quality speechâ€‘toâ€‘text via `faster-whisper` (CPUâ€‘friendly).
- Wordâ€‘level timestamps â†’ smart timeâ€‘window chunking (with overlap).
- Multiâ€‘episode semantic search using `all-MiniLM-L6-v2` embeddings in ChromaDB.
- Hybrid retrieval: BM25 + dense with Reciprocal Rank Fusion.
- Returns top hits with timestamps and episode IDs; jump-to-time hints.
- (Optional) Speaker diarization with `pyannote.audio` (requires HF token).
- (Optional) Generative answers using Groq (Llamaâ€‘3.x) or OpenAIâ€”fallback to extractive answer if no API key.
- Basic metrics in UI: ingestion time, indexing time, query latency.

## Project structure

```
podcast-rag/
â”œâ”€â”€ app.py
â”œâ”€â”€ rag_pipeline/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ transcribe.py
â”‚   â”œâ”€â”€ diarize.py
â”‚   â”œâ”€â”€ chunking.py
â”‚   â”œâ”€â”€ index.py
â”‚   â”œâ”€â”€ search.py
â”‚   â”œâ”€â”€ generate.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ packages.txt           # for Hugging Face Spaces (installs ffmpeg)
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

## Quickstart (local)

```bash
git clone <your-fork-url> podcast-rag
cd podcast-rag
python -m venv .venv && source .venv/bin/activate   # on Windows: .venv\Scripts\activate
pip install -r requirements.txt
streamlit run app.py
```

Then open the Streamlit URL, upload a few `.mp3` files, click **Ingest & Index**, and start asking questions like:
- "When did they discuss reinforcement learning?"
- "What did the guest say about funding in 2024?"

## Environment variables (optional)

Create a `.env` (or set in your hosting environment):

- `GROQ_API_KEY` â€“ to use Groq's LLM (e.g., llama-3.1-8b-instant).
- `OPENAI_API_KEY` â€“ to use OpenAI models instead.
- `HUGGINGFACE_TOKEN` â€“ to enable pyannote.audio speaker diarization.

Without these keys, the app still works in extractive mode.

## Deploy to Hugging Face Spaces

1. Create a new Space â†’ Streamlit template.
2. Upload this repo (or connect to your GitHub).
3. Ensure `packages.txt` and `requirements.txt` are included (Spaces will install ffmpeg and Python deps).
4. Set optional secrets for APIs in the Space settings.
5. The app launches automatically and persists index in `./chroma` (ephemeral on free tier).

## Evaluation (lightweight)

- Retrieval@k: The app shows topâ€‘k chunk hits per query and their scores.
- Latency: Ingestion and query runtimes are printed in the UI.
- RAGAS (optional): You can export Q/A/context triplets from the UI and run RAGAS offline.

## License

MIT
